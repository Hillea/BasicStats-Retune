[
  {
    "objectID": "StatsTestsJamovi.html#research-questions-statistical-hypotheses",
    "href": "StatsTestsJamovi.html#research-questions-statistical-hypotheses",
    "title": "Statistical Analyses in Jamovi",
    "section": "Research Questions & Statistical Hypotheses",
    "text": "Research Questions & Statistical Hypotheses\nGiven our small dataset, what could be questions that you would find interesting to answer?\nLet’s collect possible Research Questions and Hypotheses!1\n\nIf you haven’t done so yet, please open the WorkshopData.csv file in Jamovi.\n\ncollect RQs/Hyps on Flipchart, cluster them possibly in order corr-t-anova…\nLet them work in teams of two on different RQs, trying to figure out how it works - 10 min.\nPresent their solutions on main pc, talk about unclear things etc.\n\nOf course, usually you would come up with these before you collect data and not based on the dataset!",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#conduct-analyses",
    "href": "StatsTestsJamovi.html#conduct-analyses",
    "title": "Statistical Analyses in Jamovi",
    "section": "Conduct Analyses",
    "text": "Conduct Analyses\nExercise: In teams of two, please conduct an analysis that would answer a research question. You have 10 min. to try to figure it out - don’t worry about things that remain unclear for now, we will discuss them!",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#correlation",
    "href": "StatsTestsJamovi.html#correlation",
    "title": "Statistical Analyses in Jamovi",
    "section": "Correlation",
    "text": "Correlation\nCorrelations are used if you have two continuous variables and you want to check whether they have a relationship (regardless of whether one might have caused the other).\n\nRunning a correlation analysis, we test whether two variables co-vary,1 i.e. if one is high, the other variable also tends to be high (or, the opposite, low).\nThe correlation coefficient r can be between -1 (perfect negative correlation) and 1 (perfect positive correlation).\n\n\n\\[\nr = \\frac{covariance}{s_xs_y} = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{(N - 1)s_x s_y}\n\\]\nThe numerator represents the covariance between x and y, the denominator is for standardizing the measure by both SDs.",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#correlations-in-jamovi",
    "href": "StatsTestsJamovi.html#correlations-in-jamovi",
    "title": "Statistical Analyses in Jamovi",
    "section": "Correlations in Jamovi",
    "text": "Correlations in Jamovi\nLet’s have a look at how to calculate correlation analyses in Jamovi.\nClick on the Analyses tab “Regression” and then “Correlation Matrix”. You can now select two variables by adding them to the right box.\nYou can also select whether you want to include a visualization of the correlation.",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#interpreting-correlations",
    "href": "StatsTestsJamovi.html#interpreting-correlations",
    "title": "Statistical Analyses in Jamovi",
    "section": "Interpreting Correlations",
    "text": "Interpreting Correlations\nInterpreting the r statistic is a bit difficult and it depends on the field and context whether a certain correlation is strong or not. One rule of thumb is the following:",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#spearmans-rank-correlation",
    "href": "StatsTestsJamovi.html#spearmans-rank-correlation",
    "title": "Statistical Analyses in Jamovi",
    "section": "Spearman’s Rank Correlation",
    "text": "Spearman’s Rank Correlation\nSo far, we have used the Pearson’s correlation coefficient, which is by far the most used one. However, we can only use this correlation if the relationship between variables is expected to be linear!\n\nStudy hours, for example, could also be related to exam score non-linearly: The first hours studied help a lot in increasing your grade, but to get from 99% to 100% might take a lot more work. Or we might be interested in the correlation involving an ordinal variable like motivation level. In such a case (or with ordinal data), we would rather use the Spearman Rank Correlation.\n\n\nFor this analysis, Jamovi doesn’t use the raw data but ranks them in order and uses these ranks. If participant 1 was the person with the most study hours and the best exam score, they would be rank 1 for both variables. Subsequently, the ranks are correlated.",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#reporting-correlations",
    "href": "StatsTestsJamovi.html#reporting-correlations",
    "title": "Statistical Analyses in Jamovi",
    "section": "Reporting Correlations",
    "text": "Reporting Correlations\nWe usually report the correlation coefficient (e.g. r) with the degrees of freedom and the p-value:\nr(98) = .673, p &lt; .001",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#t-tests",
    "href": "StatsTestsJamovi.html#t-tests",
    "title": "Statistical Analyses in Jamovi",
    "section": "T-Tests",
    "text": "T-Tests\n\n\n\nOften, we want to know whether two groups (treatment vs. control, men vs. women…) differ on some measure.\nIn other words, our outcome variables are continuous (e.g. exam scores) and we’re interested in the effect of a categorical predictor.\nIn this case, a t-test would be our statistical analysis of choice.\n\nThere are different kinds of t-tests, depending on the relationship between the two groups:\n\nIf the two groups are unrelated, e.g. men and women, we would run an independent samples t-test.\nIf the two groups are related, e.g. first and second measurement of the same participants, we would run a paired-samples t-test.\n(Sometimes we just want to compare one group to some specific value, then we’d use the one-sample t-test.)",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#independent-samples-t-test",
    "href": "StatsTestsJamovi.html#independent-samples-t-test",
    "title": "Statistical Analyses in Jamovi",
    "section": "Independent Samples t-Test",
    "text": "Independent Samples t-Test\nLet’s first look at the common situation where we want to compare two groups. We often have two independent groups if participants are randomly assigned to one of two conditions.\nExample Research Question: Do people who drink a lot of coffee score better on an exam than those who drink less coffee?\n\nHypotheses:\n\\(H_0\\) = High caffeine intake is related to equal or lower exam scores than low caffeine intake.\n\\(H_A\\) = High caffeine intake is related to higher exam scores than low caffeine intake.",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#computing-a-2-level-factor-grouping-variable",
    "href": "StatsTestsJamovi.html#computing-a-2-level-factor-grouping-variable",
    "title": "Statistical Analyses in Jamovi",
    "section": "Computing a 2-Level Factor (Grouping Variable)",
    "text": "Computing a 2-Level Factor (Grouping Variable)\nIf we want to run a t-Test, we first need to define a variable with two levels. Caffeine intake, our grouping variable, has three levels. However, we could argue that “no coffee” could also fall in the “low caffeine intake” group. We thus first have to compute a new variable with only two levels, “low” (including no and low coffee intake) and “high”.\nExercise: Try to compute a new variable, recoding values to “low” if they are non-high.",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#independent-samples-t-test-in-jamovi",
    "href": "StatsTestsJamovi.html#independent-samples-t-test-in-jamovi",
    "title": "Statistical Analyses in Jamovi",
    "section": "Independent Samples t-Test in Jamovi",
    "text": "Independent Samples t-Test in Jamovi\nExercise: Try to figure out how to run an Independent Samples t-Test to test the influence of caffeine intake on exam scores!\n\nAnalysis -&gt; T-Tests -&gt; Independent Samples -&gt; Exam_Post as Dependent Variable -&gt; new caffeine variable as Grouping Variable.\nIt is good practice to choose Welch’s test (because it allows for the SDs of the groups to vary), as well as selecting a directed/one-sided hypothesis if possible.\nAlso, you should test your assumptions, which is really easy in Jamovi.\nFinally, it is a good idea to report the descriptives, mean difference with CI, and effect size as well.",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#reporting-an-independent-samples-t-test",
    "href": "StatsTestsJamovi.html#reporting-an-independent-samples-t-test",
    "title": "Statistical Analyses in Jamovi",
    "section": "Reporting an Independent Samples t-Test",
    "text": "Reporting an Independent Samples t-Test\n\nA one-sided Welch’s t-Test, testing whether high caffeine intake leads to better exam scores than low caffeine intake, revealed a significant difference (M_diff = 5.42, SE_diff = 2.31) between the two groups (t(78.0) = 2.35; p = .011, Cohen’s d = .485). High caffeine intake leads to higher exam scores (M = 70.7, SD = 11.2) than low caffeine intake (M = 65.3, SD = 11.2). All assumptions of the t-Test were met.",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#paired-samples-t-tests",
    "href": "StatsTestsJamovi.html#paired-samples-t-tests",
    "title": "Statistical Analyses in Jamovi",
    "section": "Paired-Samples t-Tests",
    "text": "Paired-Samples t-Tests\nOften, we measure the same participants twice: E.g. once in the treatment manipulation, once in the control condition (and the order would hopefully be counterbalanced) - or before and intervention and after.\nIf each participant appears in both groups, we have to use the paired-samples t-test.\n\nExample Research Question: Do exam scores decline over time? (This could be due to e.g. a manipulation in between.)\nHypotheses:\n\\(H_0\\) = Exam scores stay the same or improve in a re-test.\n\\(H_A\\) = Exam scores are lower in a re-test than in the first exam.",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#paired-samples-t-tests-in-jamovi",
    "href": "StatsTestsJamovi.html#paired-samples-t-tests-in-jamovi",
    "title": "Statistical Analyses in Jamovi",
    "section": "Paired-Samples t-Tests in Jamovi",
    "text": "Paired-Samples t-Tests in Jamovi\nObviously, we need to collect data with repeated measures for a paired-samples t-test. Fortunately, we already have Pre (Mid,) and Post measures of the exam scores (in a wide format).\n\n\nExercise: Try to run a paired-samples t-test comparing pre and post exam scores. We expected post scores to be lower.\n\n\nAnalyses -&gt; T-Tests -&gt; Paired Samples T-Test -&gt; add Exam_Pre and Exam_Post to the “Paired Variables” Box.\nAlso select the assumption checks, the correct directed hypothesis, and mean difference, effect size and descriptives. What do you notice?\n\n\nThe assumption of normality seems to be violated - however, the QQ-plot looks okayish (dots fall on line).\n\n\n\nNote on Long vs. wide data: For a lot of analyses, you’d need the data in long format, with several rows per participant. For this t-test in Jamovi, however, you’d need it in wide format, which means one row per participant and separate columns for exam scores pre, mid, and post measurements.\n___\nInstead of the Student’s t-test, you could run the non-parametric alternative, which is the Wilcoxon rank test and is independent of the assumption of normality.\nThis tests whether the median (ranked) difference between paired observations is zero.",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#reporting-a-paired-samples-t-test",
    "href": "StatsTestsJamovi.html#reporting-a-paired-samples-t-test",
    "title": "Statistical Analyses in Jamovi",
    "section": "Reporting a Paired-Samples t-Test",
    "text": "Reporting a Paired-Samples t-Test\nExercise: How would you report the result?\n\n\nIn contrast to what we expected, post exam scores were not lower than pre exam scores but rather higher (M_post = 73.8, SD_post = 11.7; M_pre = 67.1, SD_pre = 11.4). The one-sided paired-samples t-test was not significant (t(99) = -16.1; p = .999; M_diff = -6.66; SE_diff = 0.41, Cohen’s d = -1.61).\n\nAlternatively:\n\nIn contrast to what we expected, post exam scores were not lower than pre exam scores but rather higher (Median_post = 73.0; Median_pre = 67.7). The assumption of normality was violated, which is why a Wilcoxon rank test was conducted. This one-sided t-test was not significant (W = 25.0; p = .999; r = -.99).",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#one-way-analysis-of-variance",
    "href": "StatsTestsJamovi.html#one-way-analysis-of-variance",
    "title": "Statistical Analyses in Jamovi",
    "section": "One-Way Analysis of Variance",
    "text": "One-Way Analysis of Variance\nWith ANOVAs, we generally test whether there are differences between the groups, but not directly which groups differ from each other. (Spoiler: To do so, we need to run follow-up (t-)tests.)\nIn other words, our null hypothesis would be that e.g. caffeine has no effect on scores and all groups score the same:\n\\[H_0 = mean_n == mean_l == mean_h \\]\nIn contrast, the alternative hypothesis would be:\n\\[H_a = \\text{not all means are equal}\\]",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#analysis-of-variance",
    "href": "StatsTestsJamovi.html#analysis-of-variance",
    "title": "Statistical Analyses in Jamovi",
    "section": "Analysis of Variance?",
    "text": "Analysis of Variance?\nAlthough we want to compare means of the groups to test the hypothesis, the variance is important for this (slightly more complex) statistical test.\nThere are different relevant variances, between-group and within-group:\n\n\nTo simplify it (a lot), what we’re interested in is whether the amount that the groups differ from each other (between-group variation) is bigger than the variance within the groups.\n\n[variance –&gt; sum of squares –&gt; F]",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#one-way-anova-in-jamovi",
    "href": "StatsTestsJamovi.html#one-way-anova-in-jamovi",
    "title": "Statistical Analyses in Jamovi",
    "section": "One-Way ANOVA in Jamovi",
    "text": "One-Way ANOVA in Jamovi\nExercise: Try to figure out how to run a one-way ANOVA in Jamovi, testing whether the three levels of coffee intake lead to differences in exam scores.",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#interpreting-the-anova",
    "href": "StatsTestsJamovi.html#interpreting-the-anova",
    "title": "Statistical Analyses in Jamovi",
    "section": "Interpreting the ANOVA",
    "text": "Interpreting the ANOVA\n\nThe p-value is not significant at 𝛼 = 0.05.\n\nEven if the result would be significant, we would not know yet how exactly the groups differ. For this aim, we could have a look at the descriptive plot and run post-hoc tests. It’s usually a good idea to correct for multiple comparisons.",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#assumptions-of-one-way-anova",
    "href": "StatsTestsJamovi.html#assumptions-of-one-way-anova",
    "title": "Statistical Analyses in Jamovi",
    "section": "Assumptions of One-Way ANOVA",
    "text": "Assumptions of One-Way ANOVA\nAgain, it is a very good idea to test your assumptions to see whether you can actually interpret the results well. It is easy to know which assumptions should be met for the one-way ANOVA if you use Jamovi, because these are listed under Assumption Checks. Simply select them all.\n\nIn this case, all assumptions seem to be met: The test of normality is not significant (so no deviations from normality) and in the QQ-plot the dots also fall more or less on the line. In addition, the test of homogeneity of variances is also non-significant (which means no deviations from homogeneity).",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#reporting-the-one-way-anova",
    "href": "StatsTestsJamovi.html#reporting-the-one-way-anova",
    "title": "Statistical Analyses in Jamovi",
    "section": "Reporting the One-Way ANOVA",
    "text": "Reporting the One-Way ANOVA\nAlthough high caffeine intake leads to slightly higher exam scores than low or no caffeine intake (see table x), a one-way ANOVA indicated that there is no significant difference between the three groups (F(2, 61.3) = 2.72; p = .074). Assumptions of the one-way ANOVA were met.",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#repeated-measures-anova",
    "href": "StatsTestsJamovi.html#repeated-measures-anova",
    "title": "Statistical Analyses in Jamovi",
    "section": "Repeated measures ANOVA",
    "text": "Repeated measures ANOVA\nSimilar to the (independent- vs. paired-samples t-test), there are also different ANOVAs for independent and repeated measures. The one-way ANOVA mentioned before can only be used if the data are independent, e.g. from different participants. Often, you do have repeated (within-subjects) measures, e.g. measuring the same participant at several time points or in different conditions.\n\nIn Jamovi, we would, again, need the wide data set for this with pre, mid, and post exam scores.\nYou can then simply select ANOVA –&gt; Repeated Measures ANOVA. Specifying the variables is a bit more tricky, as you have to write down the names of the “factor levels” in the first box, and drag the respective variable to the second column.",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#reporting-the-repeated-measures-anova",
    "href": "StatsTestsJamovi.html#reporting-the-repeated-measures-anova",
    "title": "Statistical Analyses in Jamovi",
    "section": "Reporting the Repeated Measures ANOVA",
    "text": "Reporting the Repeated Measures ANOVA\nBefore we report the ANOVA, we would again check the assumptions: Sphericity tests whether the variances of the differences between the conditions are equal. It should be non-significant, but if it is significant, we could add a sphericity correction to the degrees of freedom.\n\nThe results show that there is indeed a difference between the time points in exam scores (F(2, 198) = 143, p &lt; .001). It’s also good practice to report the effect size, (partial) eta squared.",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#post-hoc-tests",
    "href": "StatsTestsJamovi.html#post-hoc-tests",
    "title": "Statistical Analyses in Jamovi",
    "section": "Post-Hoc Tests",
    "text": "Post-Hoc Tests\nAgain, we still don’t know which time points differ from each other, for which we would run post-hoc tests (e.g. Tukey-corrected for multiple comparisons):\n\nWe can see that all time points significantly differ from each other, e.g. pre vs. mid (t(99) = -9.43; p &lt; .001).\n\nUnder estimated marginal means you can get a plot of the results:",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#adding-several-factors",
    "href": "StatsTestsJamovi.html#adding-several-factors",
    "title": "Statistical Analyses in Jamovi",
    "section": "Adding several factors",
    "text": "Adding several factors\nOne big advantage of ANOVAs (compared to t-tests) is that you can add several variables in it together to analyze their main effects as well as interactions between variables. For example, you might be interested in whether there are differences in caffeine intake (between-measures!) over time in the exam scores (within-variable!).\n\nTo do so in Jamovi, you would simply add “caffeine intake” to the “Between Subject Factor” box.\nIf you then check out the output, you will see that there is a main effect for between-subjects and the interaction added (which are both non-significant).",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#factorial-anova",
    "href": "StatsTestsJamovi.html#factorial-anova",
    "title": "Statistical Analyses in Jamovi",
    "section": "Factorial ANOVA",
    "text": "Factorial ANOVA\nSimilary, we might be interested in the effects (and interaction) of two (or more) between-subjects variables on some outcome, such as whether caffeine intake and gender have an effect on exam scores.\nE.g.: We could imagine that coffee has a stronger effect for men on exam scores than for women.\n\nTo analyze this, we don’t need to run a repeated measures ANOVA, but can rather use a normal or factorial ANOVA. In Jamovi, we specify the dependent or outcome variable and the predictors as fixed factors.\nAs with previous tests, we can test the assumptions, specify the precise model, run post-hoc tests and get plots (estimated marginal means).\nIn this example, none of the effects reach significance.",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#regressionlinear-model",
    "href": "StatsTestsJamovi.html#regressionlinear-model",
    "title": "Statistical Analyses in Jamovi",
    "section": "Regression/Linear Model",
    "text": "Regression/Linear Model\nSo far, we have mainly looked into using grouping variables (or categorical factors) as predictors. Only in correlation, we had two continuous variables. Sometimes we are interested in the effects (and interactions) of several (between-subjects) continuous and/or categorical variables on a (continuous) predictor. In this case, we can use an analysis that is actually like a swiss army knife: The regression analysis or linear model.",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "StatsTestsJamovi.html#interpreting-regression-output",
    "href": "StatsTestsJamovi.html#interpreting-regression-output",
    "title": "Statistical Analyses in Jamovi",
    "section": "Interpreting Regression Output",
    "text": "Interpreting Regression Output\nWhile discussing the details of regression models is beyond the scope of this workshop, just note that the output looks a bit different than what we’re used from the ANOVAs. You get a t-value and a p-value for each effect, including comparisons of groups (one group to a reference level, e.g. “none” coffee intake). You can usually interpret this outcomes, but check out this chapter for details.",
    "crumbs": [
      "Statistical Analyses in Jamovi"
    ]
  },
  {
    "objectID": "Jamovi.html#install-jamovi",
    "href": "Jamovi.html#install-jamovi",
    "title": "Getting Started with Jamovi",
    "section": "Install Jamovi",
    "text": "Install Jamovi\n\n\n\nIf you haven’t done so, please install Jamovi: https://www.jamovi.org/.",
    "crumbs": [
      "Getting Started with Jamovi"
    ]
  },
  {
    "objectID": "Jamovi.html#first-steps",
    "href": "Jamovi.html#first-steps",
    "title": "Getting Started with Jamovi",
    "section": "First Steps",
    "text": "First Steps\nIf you open Jamovi, it should look like this:",
    "crumbs": [
      "Getting Started with Jamovi"
    ]
  },
  {
    "objectID": "Jamovi.html#load-a-dataset",
    "href": "Jamovi.html#load-a-dataset",
    "title": "Getting Started with Jamovi",
    "section": "Load a Dataset",
    "text": "Load a Dataset\nOpen the file WorkshopData.csv by clicking the three stripes on the upper left, “open”, “browse” and searching for and selecting the file in the file explorer.",
    "crumbs": [
      "Getting Started with Jamovi"
    ]
  },
  {
    "objectID": "Jamovi.html#variables",
    "href": "Jamovi.html#variables",
    "title": "Getting Started with Jamovi",
    "section": "Variables",
    "text": "Variables\nEach column in the “Data” part represents a variable, each row a “case” or participant.\nIf you double-click on the header/name of the variable, a small menu opens where you can see the meaurement type etc. Jamovi guesses these measurement and data types, but you could also change them. Similarly, you can also change the name of the variable.\n\n\nExercise: Please look through the variables and check whether the measurement type, the data type and the levels are correct. Discuss if you are not sure.",
    "crumbs": [
      "Getting Started with Jamovi"
    ]
  },
  {
    "objectID": "Jamovi.html#adding-variables",
    "href": "Jamovi.html#adding-variables",
    "title": "Getting Started with Jamovi",
    "section": "Adding Variables",
    "text": "Adding Variables\nYou can also add a new variable by either clicking on the header of an empty column or by clicking “add” in the Variables or Data menu. You then have the choice between adding a new variable, compute a variable using existing variables, or transforming an existing variable. The latter can also be achieved by clicking “Compute” or “Transform” in the Variables or Data menu.\n\nComputing or transforming a variable can be helpful to get a different version or e.g. a difference or average between variables.\nExercise: Check out the different functions available for computing or transforming a variable by clicking on the little fx.\n\n\nExercise 2: Make a new variable which is the difference between Exam_Pre and Exam_Post.",
    "crumbs": [
      "Getting Started with Jamovi"
    ]
  },
  {
    "objectID": "Jamovi.html#add-ons",
    "href": "Jamovi.html#add-ons",
    "title": "Getting Started with Jamovi",
    "section": "Add-ons",
    "text": "Add-ons\nJamovi allows you to run basic analyses. Add-ons can increase the number of possible analyses. These are so-called modules for specific purposes that other people created and that you can add to your version of Jamovi.\nIn the Analysis menu, you can find a + sign on the right. If you click on it, you can browse the Jamovi library and search for modules that might be helpful for you. The modules range from specific analyses to data transformation to visualization.",
    "crumbs": [
      "Getting Started with Jamovi"
    ]
  },
  {
    "objectID": "Jamovi.html#quitting-jamovi",
    "href": "Jamovi.html#quitting-jamovi",
    "title": "Getting Started with Jamovi",
    "section": "Quitting Jamovi",
    "text": "Quitting Jamovi\nIf you quit Jamovi, you can save your dataset (preferably under a different name if you made changes) and/or the results. One advantage of Jamovi is that it is possible to save both data and (annotated) results in the same file, which makes it easier to later understand the results. The output of Jamovi is also already nicely formatted.",
    "crumbs": [
      "Getting Started with Jamovi"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Basic Statistics Workshop Fall 2025",
    "section": "",
    "text": "Welcome to the website of the workshop “Basic statistics” conducted during the Fall School 2025 of the TRR ReTune. The workshop will be given by Dr. Lea Hildebrandt.\nPlease use the navigation on the left to select the slides for each session (recommendation: Open in new tab).\nBefore the day of the workshop, please download and install Jamovi: https://www.jamovi.org/.\nYou can download the dataset we’ll use here. Click on the little download button on the top right:"
  },
  {
    "objectID": "index.html#schedule",
    "href": "index.html#schedule",
    "title": "Basic Statistics Workshop Fall 2025",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nTime\nTopic\n\n\n\n\n9:00 - 9:20\nIntro: Why Statistics?\n\n\n9:20 - 9:40\nGetting started with Jamovi\n\n\n9:40 - 10:00\nDescriptive Statistics & Data Visualization\n\n\n10:00 - 10:45\nProbability & Sampling, Hypothesis Testing\n\n\n10:45 - 11:00\nCoffee Break\n\n\n11:00 - 12:45\nStatistical Tests in Jamovi"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "DescViz.html#mean-median-mode",
    "href": "DescViz.html#mean-median-mode",
    "title": "Describing and Visualizing Data",
    "section": "Mean, Median, Mode",
    "text": "Mean, Median, Mode\n\nMean: Add up all values and divide by number of values:\n\\[\n(10+2+3+4+5)/5 = 4.8\n\\]\n\n\nMedian: The middle value of all sorted values, e.g 4:\n\\[\n2,3,4,5,10\n\\]\n\n\nMode: The most common value in the dataset, e.g. 2 in this example:\n\\[\n2,5,3,4,2,10\n\\]\n\n\n\n\n\nnominal data = mode (not meaningful values)\nordinal = rather use median than mean\ninterval/ratio = both mean and median are fine, mean uses all the information but is sensitive to extreme outliers",
    "crumbs": [
      "Describing and Visualizing Data"
    ]
  },
  {
    "objectID": "DescViz.html#descriptive-statistics-in-jamovi",
    "href": "DescViz.html#descriptive-statistics-in-jamovi",
    "title": "Describing and Visualizing Data",
    "section": "Descriptive Statistics in Jamovi",
    "text": "Descriptive Statistics in Jamovi\nExercise: Try to figure out the mean, median, and mode of Age and Exam_Post.\n\nExercise 2: Get the mean age for each gender separately.",
    "crumbs": [
      "Describing and Visualizing Data"
    ]
  },
  {
    "objectID": "DescViz.html#variability",
    "href": "DescViz.html#variability",
    "title": "Describing and Visualizing Data",
    "section": "Variability",
    "text": "Variability\nIn addition to a value for “the middle of the data”, another important measure would be the “spread” or variability.\nThere are different measures for variability:\n\nRange: Biggest value minus smallest value:\n\\[\n-100, 2,3,4,5,6,7,8,9,10 =&gt; 110\n\\]\n\n\n\nnot very robust!\n\n\n\nInterquartile range (IQR): Difference between 25th and 75th percentile. Percentiles are the smallest number such that e.g. 25% of the data are less than that number. The median is the 50th percentile!\n\n\n\n25th and 75th percentile can be easily calculated in Jamovi: Exploration -&gt; Descriptives -&gt; Statistics -&gt; click “Percentiles”.\nThe IQR is the range covered by the “middle half” of the data, often used together with the median.",
    "crumbs": [
      "Describing and Visualizing Data"
    ]
  },
  {
    "objectID": "DescViz.html#variability-2",
    "href": "DescViz.html#variability-2",
    "title": "Describing and Visualizing Data",
    "section": "Variability 2",
    "text": "Variability 2\nMean absolute value: the “typical deviation” from the mean (or median):\n\nCalculate the mean of the data:\n\\[\n(10+2+3+4+5)/5 = 4.8\n\\]\nCalculate each absolute deviation from each data point to this mean:\n\\[\n10 - 4.8 = 5.2;\\] \\[\n2 - 4.8 = |-2.8| = 2.8;\\] \\[\n3 - 4.8 = |-1.8| = 1.8;\\] \\[\n4 - 4.8 = |-0.8| = 0.8;\\] \\[\n5 - 4.8 = 0.2\n\\]\nCalculate the mean of these deviation:\n\\[\n(5.2 + 2.8 + 1.8 + 0.8 +0.2)/5 = 2.16\n\\]",
    "crumbs": [
      "Describing and Visualizing Data"
    ]
  },
  {
    "objectID": "DescViz.html#variability-3",
    "href": "DescViz.html#variability-3",
    "title": "Describing and Visualizing Data",
    "section": "Variability 3",
    "text": "Variability 3\nIt is often better (for mathematical reasons) to use the squared deviation instead of the absolute deviation from the mean.\nThis is called the variance:\n\nCalculate the mean of the data:\n\\[ (10+2+3+4+5)/5 = 4.8 \\]\nCalculate each squared deviation from each data point to this mean:\n\\[ (10 - 4.8)^2 = 27.04;\\] \\[ (2 - 4.8)^2 = 7.84;\\] \\[ (3 - 4.8)^2 = 3.24;\\] \\[ (4 - 4.8)^2 = 0.64;\\] \\[ (5 - 4.8)^2 = 0.04 \\]\nCalculate the mean of these deviation, but use N-1 as the denominator:\n\\[ (27.04 + 7.84 + 3.24 + 0.64 +0.02)/4 = 9.695 \\]\nOr, in mathematical formulation:\n\\[\nVAR(X) = \\frac{1}{N-1}\\sum_{i=1}^{N}(X_i-\\bar{X})^2\n\\]",
    "crumbs": [
      "Describing and Visualizing Data"
    ]
  },
  {
    "objectID": "DescViz.html#variability-4",
    "href": "DescViz.html#variability-4",
    "title": "Describing and Visualizing Data",
    "section": "Variability 4",
    "text": "Variability 4\nThe problem with the variance is: It is completely uninterpretable as it is in the original unit squared.\nSolution: Take the square root of the variance to bring it back to the original units!\nThis is called the standard deviation or root mean squared deviation (RMSD):\n\\[\nSD(X) = \\sqrt{\\frac{1}{N-1}\\sum_{i=1}^{N}(X_i-\\bar{X})^2}\n\\]\n\nRule of thumb: 68% of the data fall within 1 SD of the mean, 95% within 2 SD, 99,7% within 3 SD.\nThe SD is often used, especially in combination with the mean.",
    "crumbs": [
      "Describing and Visualizing Data"
    ]
  },
  {
    "objectID": "DescViz.html#standard-scores",
    "href": "DescViz.html#standard-scores",
    "title": "Describing and Visualizing Data",
    "section": "Standard Scores",
    "text": "Standard Scores\nOften, it is helpful to standardize scores to make them comparable e.g. across samples and test/measurement methods or even different variables. It transforms the data to state where an observation falls relative to its own population.\n\\[ score_{standardized} = \\frac{score - mean}{SD}\\]",
    "crumbs": [
      "Describing and Visualizing Data"
    ]
  },
  {
    "objectID": "DescViz.html#principles-of-plotting",
    "href": "DescViz.html#principles-of-plotting",
    "title": "Describing and Visualizing Data",
    "section": "Principles of Plotting",
    "text": "Principles of Plotting\n\nAnatomy of a plot: We have an x-axis (horizontal) and a y-axis (vertical).\nIt’s always a good idea to show the data.\nAvoid clutter.\nDon’t distort the data.\nAccount for perceptual limitations (e.g. color blindness, pie charts…)\n\n\nGo through plots in Jamovi",
    "crumbs": [
      "Describing and Visualizing Data"
    ]
  },
  {
    "objectID": "DescViz.html#histograms",
    "href": "DescViz.html#histograms",
    "title": "Describing and Visualizing Data",
    "section": "Histograms",
    "text": "Histograms\nHistograms help you to get an impression of how your data of a variable is distributed. They are used for continuous data (interval or ratio scale).\n\nAll values are divided into bins (choice of bins happens automatically in stats software),\nThe number of observations that fall in each bin are counted (frequency). This is the height of each vertical bar.\n\n\n\nIn Jamovi: Exploration -&gt; Descriptives -&gt; select “histogram” check box.",
    "crumbs": [
      "Describing and Visualizing Data"
    ]
  },
  {
    "objectID": "DescViz.html#boxplots",
    "href": "DescViz.html#boxplots",
    "title": "Describing and Visualizing Data",
    "section": "Boxplots",
    "text": "Boxplots\nAlso suited for continuous data, they depict the median, IQR, and range of the data in a compact way (and several variables can be easily plotted next to each other and thus compared).\n\nThick line in the middle of the box is the median, the upper and lower sides of the box are the IQR and the end of the vertical line is the range - but max 1.5 IQR. Every data point outside of this range is shown as dot (here labelled) and may be considered an outlier.\nIn Jamovi: Exploration -&gt; Descriptives -&gt; select “Box plot” check box.",
    "crumbs": [
      "Describing and Visualizing Data"
    ]
  },
  {
    "objectID": "DescViz.html#violin-plots",
    "href": "DescViz.html#violin-plots",
    "title": "Describing and Visualizing Data",
    "section": "Violin Plots",
    "text": "Violin Plots\nA violin plot is similar to a boxplot but also shows the density (as a mirrored vertical curve). You may also add the raw data as well as a boxplot points to the plot, which increases transparency.",
    "crumbs": [
      "Describing and Visualizing Data"
    ]
  },
  {
    "objectID": "DescViz.html#multiple-box-plots",
    "href": "DescViz.html#multiple-box-plots",
    "title": "Describing and Visualizing Data",
    "section": "Multiple (Box) Plots",
    "text": "Multiple (Box) Plots\nAs mentioned, it can make sense to visualize several variables - or the same variable for different groups - next to each other. This is called a grouped or dodged plot.\n\nYou can easily achieve this in Jamovi, if you use a grouping variable such as gender in the “split by” box.",
    "crumbs": [
      "Describing and Visualizing Data"
    ]
  },
  {
    "objectID": "DescViz.html#bar-graphs",
    "href": "DescViz.html#bar-graphs",
    "title": "Describing and Visualizing Data",
    "section": "Bar Graphs",
    "text": "Bar Graphs\nA bar graph looks on first sight similar to a histogram, but it does not show the distribution but rather one value (usually counts) for different groups.\n\nYou can also generate bar charts for continuous data that are not counts - but this is not advised. In this case, it will display the mean and the error bars will be the standard error (another measure of variability).",
    "crumbs": [
      "Describing and Visualizing Data"
    ]
  },
  {
    "objectID": "DescViz.html#scatterplots",
    "href": "DescViz.html#scatterplots",
    "title": "Describing and Visualizing Data",
    "section": "Scatterplots",
    "text": "Scatterplots\nIf you have two continuous variables, you may want to make a scatter plot. This can be achieved via the correlation analysis (more to follow) or using the Scatr module.",
    "crumbs": [
      "Describing and Visualizing Data"
    ]
  },
  {
    "objectID": "Intro.html#hello",
    "href": "Intro.html#hello",
    "title": "Basic Statistics",
    "section": "Hello!",
    "text": "Hello!\n\n\nWho are you?\n\nWhat is your background?\nDo you have experience with data analysis?\nWhat’s your attitude towards statistics? Statistical confessions?",
    "crumbs": [
      "Basic Statistics"
    ]
  },
  {
    "objectID": "Intro.html#what-to-expect-today",
    "href": "Intro.html#what-to-expect-today",
    "title": "Basic Statistics",
    "section": "What to expect today?",
    "text": "What to expect today?\n\nIf you haven’t done so, please download Jamovi (www.jamovi.org).\nSlides can be found here: https://hillea.github.io/BasicStats-Retune/\nTextbooks this workshop is based on/that can be used to read up:\n\nLearning Statistics with Jamovi\nStatistical Thinking for the 21st Century\n\nRather brief overview!",
    "crumbs": [
      "Basic Statistics"
    ]
  },
  {
    "objectID": "Intro.html#schedule",
    "href": "Intro.html#schedule",
    "title": "Basic Statistics",
    "section": "Schedule",
    "text": "Schedule\n\n\n\nTime\nTopic\n\n\n\n\n9:00 - 9:20\nIntro: Why Statistics?\n\n\n9:20 - 9:40\nGetting started with Jamovi\n\n\n9:40 - 10:00\nDescriptive Statistics & Data Visualization\n\n\n10:00 - 10:45\nProbability & Sampling, Hypothesis Testing\n\n\n10:45 - 11:00\nCoffee Break\n\n\n11:00 - 12:45\nStatistical Tests in Jamovi",
    "crumbs": [
      "Basic Statistics"
    ]
  },
  {
    "objectID": "Intro.html#why-is-it-important-that-you-know-statistics",
    "href": "Intro.html#why-is-it-important-that-you-know-statistics",
    "title": "Basic Statistics",
    "section": "Why is it important that YOU know statistics?",
    "text": "Why is it important that YOU know statistics?\n\n\nYou’re doing a PhD!\nWe live in an increasingly data-centric world\nFacts & data literacy matter more than ever!\n\nYou should be able to call bullshit (https://www.callingbullshit.org/)\n\n\n\n\n\nYou’re doing a PhD!\n\nResearch = Reading & understanding papers (esp. the analyses)\nDesigning your own experiments, analyze data, interpret results\n\nWe live in an increasingly data-centric world\n\nKnowing how to wrangle and analyze data is a valuable skill\n\nFacts & data literacy matter more than ever!\n\nFake News, “Lying with stats”, Reproducibility Crisis\nBeing able to call bullshit (https://www.callingbullshit.org/)\n\n“I only believe in statistics that I doctored myself” ― Winston S. Churchill\n\nHowever: “It is easy to lie with statistics, but easier to lie without them” ― Frederick Mosteller\n\n\n\n\n\n\n\n\n\n\n\n\n\nhttps://www.reuters.com/fact-check/misleading-data-used-claim-covid-vaccines-do-more-harm-than-good-2024-03-21\n\n\n\nhttps://en.wikipedia.org/wiki/John_Bohannon#Intentionally_misleading_chocolate_study",
    "crumbs": [
      "Basic Statistics"
    ]
  },
  {
    "objectID": "Intro.html#what-can-statistics-do-for-us",
    "href": "Intro.html#what-can-statistics-do-for-us",
    "title": "Basic Statistics",
    "section": "What can Statistics Do For Us?",
    "text": "What can Statistics Do For Us?\n\nDescribe patterns by summarizing/breaking down data (“descriptive statistics”)\nDecide whether one thing is better than another, given the uncertainty (“inferential statistics”)\nPredict how other people would “behave” (generalize to new observations)\n\n\ndescribe: not useful to look at every single data point/person, but we need s.th. like tendencies/trends…",
    "crumbs": [
      "Basic Statistics"
    ]
  },
  {
    "objectID": "Intro.html#what-are-data",
    "href": "Intro.html#what-are-data",
    "title": "Basic Statistics",
    "section": "What are Data?",
    "text": "What are Data?\n\nWhat do you think are data?\n\n\n\nqualitative vs. quantitative\n\nqualitative?\n\nopen questions, descriptions… can potentially be coded into categories\n\nquantitative?\n\nnumeric, can be averaged etc.\n\n\n\n\n\nChat → after showing slide! Come up with examples for “Data”\nCollect: Do you have ideas? What are data you encounter in your lives/work etc? What are differences between these data?",
    "crumbs": [
      "Basic Statistics"
    ]
  },
  {
    "objectID": "Intro.html#what-are-data-2",
    "href": "Intro.html#what-are-data-2",
    "title": "Basic Statistics",
    "section": "What are Data? (2)",
    "text": "What are Data? (2)\n\n\nData types\n\ncharacter/string: text (qualitative)\nfactors/categories\ntypes of numbers (quantitative)\n\nbinary: 0 or 1, TRUE or FALSE (logical)\nintegers: whole numbers\nreal numbers: decimals/fractions\n\n\ndiscrete vs. continuous\n\ndiscrete: finite set of particular values (0 or 1, scale from 1 to 10)\ncontinuous: real numbers that fall into particular range (e.g., brain activity, visual analoge scale)\n\nWhat data type is eye color?\n\n\n\neye color can be categorical (e.g., “brown”, “blue”, “green”) or numerical (wave length in nm)",
    "crumbs": [
      "Basic Statistics"
    ]
  },
  {
    "objectID": "Intro.html#what-is-a-data-set",
    "href": "Intro.html#what-is-a-data-set",
    "title": "Basic Statistics",
    "section": "What is a Data Set?",
    "text": "What is a Data Set?\n\na collection of data\nusually organized into rows and columns (like an excel spreadsheet)\n\nrows: participants/animals/cells…\ncolumns: variables!\n\neach variable contains one data type\n\ntable cells = unique observations of variables per participant etc.\n\n\n\n\npossibly go through columns and ask for data types?",
    "crumbs": [
      "Basic Statistics"
    ]
  },
  {
    "objectID": "Intro.html#what-makes-a-good-measurement",
    "href": "Intro.html#what-makes-a-good-measurement",
    "title": "Basic Statistics",
    "section": "What Makes a Good Measurement?",
    "text": "What Makes a Good Measurement?\n\n\n\nWhat is being measured?\n\nconstructs vs. proxies: need to be well-defined! (Difficult)\nmeasurement error\n\nrandom: e.g., variation in reaction times of same participant across trials\nsystematic: e.g., miscalibrated eye-tracking device\n\n\nDo we have a “gold standard” to compare the measurement to?\n\n\n\nGroup work/brainstorm:\n\nWhat are problems?\nWhich kind of errors/when is data NOT good\nhow can we minimize error?",
    "crumbs": [
      "Basic Statistics"
    ]
  },
  {
    "objectID": "Intro.html#reliability-validity",
    "href": "Intro.html#reliability-validity",
    "title": "Basic Statistics",
    "section": "Reliability & Validity",
    "text": "Reliability & Validity\nData that we collected should be both:\n\nreliable: stable and consistent over time\nvalid: measuring the construct we’re interested in\n\n\nReliability & Validity",
    "crumbs": [
      "Basic Statistics"
    ]
  },
  {
    "objectID": "Intro.html#summarizing-data",
    "href": "Intro.html#summarizing-data",
    "title": "Basic Statistics",
    "section": "Summarizing Data",
    "text": "Summarizing Data\n\n\nThrowing away (some of the) information!\n\nextract the quintessence of the data (important for forming models)\nmake predictions\n\nCounts, frequencies, percentages, averages",
    "crumbs": [
      "Basic Statistics"
    ]
  },
  {
    "objectID": "Probability.html#frequentist-vs.-bayesian-view",
    "href": "Probability.html#frequentist-vs.-bayesian-view",
    "title": "Statistical Theory",
    "section": "Frequentist vs. Bayesian View",
    "text": "Frequentist vs. Bayesian View\nFrequentist: For the next 100 days, the weather forecast predicts 60% rain, which means that on 60 days, it will rain. (The probability gets closer to its real value over time).\nBayesian: The weather forecast predicts 60% rain for the next 100 days, which means that I expect it to rain tomorrow with 60% chance (degree of belief).",
    "crumbs": [
      "Statistical Theory"
    ]
  },
  {
    "objectID": "Probability.html#probability-distributions",
    "href": "Probability.html#probability-distributions",
    "title": "Statistical Theory",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nIf we assign a certain probability to each possible outcome, we have a probability distribution!\n\n\nProbability distributions are central to determine how likely an observed outcome is given a certain model (i.e. that specific probability distribution).\nThere are different probability distributions that are often used.",
    "crumbs": [
      "Statistical Theory"
    ]
  },
  {
    "objectID": "Probability.html#probability-distributions-1",
    "href": "Probability.html#probability-distributions-1",
    "title": "Statistical Theory",
    "section": "Probability Distributions",
    "text": "Probability Distributions\nThere are a number of different probability distributions, such as the binomial distribution, the normal or gaussian distribution, poisson distribution etc.\n\nFor some specific analysis, you might need specific knowledge about these different distributions. For today, we will limit it to understanding the normal distribution, because it (or very similar distributions) are central to statistics.",
    "crumbs": [
      "Statistical Theory"
    ]
  },
  {
    "objectID": "Probability.html#the-normal-distribution",
    "href": "Probability.html#the-normal-distribution",
    "title": "Statistical Theory",
    "section": "The Normal Distribution",
    "text": "The Normal Distribution\nThe normal distribution is very common in statistics (i.e., in the real world). It (roughly) reflects the probability of any value occurring for a continuous variable, such as height.\n\n\n\nNormal distribution of height\n\n\n\nThe normal distribution is always symmetrical\n=&gt; equal probability of observations above and below the mean.\n=&gt; the mean, median, and mode are all equal!\n\n\n\n\n\n\n\nDifferent means shift the normal\ndistribution along the x-axis.\n\n\n\n\n\n\nDifferent SDs lead to wider (higher SD)\nor narrower (lower SD) distributions.",
    "crumbs": [
      "Statistical Theory"
    ]
  },
  {
    "objectID": "Probability.html#the-normal-distribution-1",
    "href": "Probability.html#the-normal-distribution-1",
    "title": "Statistical Theory",
    "section": "The Normal Distribution",
    "text": "The Normal Distribution\nAs discussed earlier (see SD), in the normal distribution, we know well how much of the probability falls in certain regions of the “normal curve”. This is very helpful when we later talk about Hypothesis Testing.",
    "crumbs": [
      "Statistical Theory"
    ]
  },
  {
    "objectID": "Probability.html#how-do-we-sample",
    "href": "Probability.html#how-do-we-sample",
    "title": "Statistical Theory",
    "section": "How Do We Sample?",
    "text": "How Do We Sample?\nThe sample needs to be representative of the entire population, that’s why it’s critical how we select the individuals.\nThink about examples of non-representative samples!\n\nRepresentative: Every member of the population has an equal chance of being selected.\nIf non-representative: sample statistic is biased, its value is (systematically) different from the true population value (parameter).\n(But talking about bias is mostly a theoretical discussion: Usually we of course don’t know the population parameter and thus cannot compare our estimate with it! Otherwise we wouldn’t need to sample.)\n\nNon-representative: pollster calls people from list of Democratic party, or from rich neighborhood, or only uses psychology students :D\nThink about “your” sample, how could this be non-representative?",
    "crumbs": [
      "Statistical Theory"
    ]
  },
  {
    "objectID": "Probability.html#different-ways-of-sampling",
    "href": "Probability.html#different-ways-of-sampling",
    "title": "Statistical Theory",
    "section": "Different Ways of Sampling",
    "text": "Different Ways of Sampling\n\nwithout replacement: Once a member of the population is sampled, they are not eligible to be sampled again. This is the most common variant of sampling.\nwith replacement: After a member of the population has been sampled, they are put back into the pool and could potentially be sampled again. This usually happens out of accident or by necessity (cf. Bootstrapping)\n\n\nBut we also have:\n\nstratified sampling, snowball sampling, convenience sampling…",
    "crumbs": [
      "Statistical Theory"
    ]
  },
  {
    "objectID": "Probability.html#population-parameters-and-sample-statistics",
    "href": "Probability.html#population-parameters-and-sample-statistics",
    "title": "Statistical Theory",
    "section": "Population Parameters and Sample Statistics",
    "text": "Population Parameters and Sample Statistics\nThe population parameter is the actual (e.g.) mean and SD in the whole population. It also refers to the parameters of our underlying probability distribution. For example, it is widely accepted that the average IQ is 100 with a SD of 15. The population parameter is usually the “thing of interest” you want to gain information about. It is also a parameter you make assumptions about (more on that later).\nThe sample statistics are (e.g.) the mean and SD of our actual data. If we measured IQ scores of 100 participants, the mean might be 98.5 and the SD 15.9.\n\nIt is likely that our sample statistic differs slightly from the population parameter. This is called the sampling error.\nIf we collect multiple samples, the sample statistic will always differ slightly. If we combine all those sample statistics, we can approximate the sampling distribution.\nOf course, we want to minimize the sampling error and get a good estimate of the population parameter!\n\n\nDiscuss: What do you think, how can we get a good estimate/minimize sampling error?",
    "crumbs": [
      "Statistical Theory"
    ]
  },
  {
    "objectID": "Probability.html#the-law-of-large-numbers",
    "href": "Probability.html#the-law-of-large-numbers",
    "title": "Statistical Theory",
    "section": "The Law of Large Numbers",
    "text": "The Law of Large Numbers\nIf you measure the IQ of 1 person, you might get an score of 120.\nIf you collect IQ scores of 10 participants, the mean might be 95 and the SD 11.\nIf you collected data of 10.000 individuals, the mean will be very close to 100 and the SD very close to 15.\n\nThe more observations, the better the descriptive statistics describe the underlying population parameter.\n\n\nBUT: While it would of course be nice to have giant samples (i.e. the population) to know the exact parameters, it is usually impossible to collect that much data. However, it is also not necessary, because even with smaller samples, we can estimate the population parameters.\n\nExample with different subsets of participants?",
    "crumbs": [
      "Statistical Theory"
    ]
  },
  {
    "objectID": "Probability.html#the-central-limit-theorem",
    "href": "Probability.html#the-central-limit-theorem",
    "title": "Statistical Theory",
    "section": "The Central Limit Theorem",
    "text": "The Central Limit Theorem\nIf we repeated an experiment with a (largish but limited) sample size, the means of these repeated samples would together form a new distribution where the mean of that distribution is close to the population mean.\n\nThis is called the Central Limit Theorem (CLT), a fundamental (and often misunderstood) concept of statistics.\nCLT: With larger sample sizes, the sampling distribution of sample means will become more and more normally distributed, even if the population distribution is not!\nNormal distributions form the basis of many statistical tests!",
    "crumbs": [
      "Statistical Theory"
    ]
  },
  {
    "objectID": "Probability.html#confidence-intervals",
    "href": "Probability.html#confidence-intervals",
    "title": "Statistical Theory",
    "section": "Confidence Intervals",
    "text": "Confidence Intervals\nAs we use our sample mean (and adjusted SD) as our best estimates for the population mean (and SD), it is a good idea to indicate how confident we are in these estimates.\nFor this we calculate the confidence interval, which denotes the e.g. 95% probability that the sample mean lies within 1.96 standard errors of the population mean.\n\n\nIt does not mean, that the true population mean lies with 95% probability within the sample’s confidence interval!\n\n\nIf we replicated the experiment with a different sample, 95% of CIs would contain the true population mean. (Still, it is a good indicator of our uncertainty around an estimate!)",
    "crumbs": [
      "Statistical Theory"
    ]
  },
  {
    "objectID": "Probability.html#null-hypothesis-significance-testing-nhst",
    "href": "Probability.html#null-hypothesis-significance-testing-nhst",
    "title": "Statistical Theory",
    "section": "Null-hypothesis significance testing (NHST)",
    "text": "Null-hypothesis significance testing (NHST)\nInstead of testing how likely our statistical hypothesis is,\nwe take the exact opposite of what we’re expecting and test how unlikely it is:\n\nHigh ≦ low caffeine intake (null hypothesis).\n\nSteps of NHST:\n\nWe take the hypothesis (treatment = lower X than control) and negate it (Treatment not lower/equal X compared to control). This is our null hypothesis, \\(H_0\\).\nThen we look at the data and determine how likely they would be if the null hypothesis were true.\nI.e., we want to know the conditional probability: \\(P(Data|H_0)\\)\nIf the data are very unlikely we reject the null hypothesis in favor of the alternative hypothesis \\(H_a\\) (our hypothesis).\n(If the data are not very unlikely, we stick with - or fail to reject - the null hypothesis.)",
    "crumbs": [
      "Statistical Theory"
    ]
  },
  {
    "objectID": "Probability.html#errors-in-decision-making",
    "href": "Probability.html#errors-in-decision-making",
    "title": "Statistical Theory",
    "section": "Errors in Decision Making",
    "text": "Errors in Decision Making\nStatistical testing means making decisions under uncertainty in a messy world –&gt; mistakes can happen.\nThe goal is to minimize these errors.\n\n\nType I error: incorrectly rejecting the null hypothesis.\nWe want to minimize this error to be below a probability of ⍺ = 0.05.\nThis means that we accept up to 5% wrong decisions if we have extreme test statistics (e.g. a huge difference in exam scores between high and low caffeine intake groups).\n\n\nType II error: incorrectly accepting the null hypothesis to be true when it is, in fact, wrong.\nWe also want to control this, although it is often secondary. This error rate (𝛽) is related to the power (1 - 𝛽) of the test, which can be increased with e.g. higher sample size.",
    "crumbs": [
      "Statistical Theory"
    ]
  },
  {
    "objectID": "Probability.html#test-statistic",
    "href": "Probability.html#test-statistic",
    "title": "Statistical Theory",
    "section": "Test Statistic",
    "text": "Test Statistic\nIn general, we want to relate an effect (e.g., a mean or a difference of means) to the amount of uncertainty in the data.\n\nWe fit a model (e.g. a t-test) to the data which provides a test statistic (e.g. a t-value) as the amount of evidence in favor of our alternative hypothesis \\(H_a\\) relative to the variability in the data.\n\n\nWe can then compare our test statistic to the probability distribution if the null hypothesis was true and determine how likely this test statistic is.",
    "crumbs": [
      "Statistical Theory"
    ]
  },
  {
    "objectID": "Probability.html#fit-a-model",
    "href": "Probability.html#fit-a-model",
    "title": "Statistical Theory",
    "section": "Fit a Model",
    "text": "Fit a Model\nIn this example, we need a test statistic that tests the difference between two (independent) means (we have one exam score mean for each group): The t statistic.\n\\[t = \\frac{\\bar{X_1} - \\bar{X_2}}{\\sqrt{\\frac{S_1^2}{n_1} + \\frac{S_2^2}{n_2}}}\\]\n\\(\\bar{X_1}\\) and \\(\\bar{X_2}\\) are the means of the two group, \\(S_1^2\\) and \\(S_2^2\\) are the estimated variances of the groups,\n\\(n_1\\) and \\(n_2\\) are the sizes of the two groups.",
    "crumbs": [
      "Statistical Theory"
    ]
  },
  {
    "objectID": "Probability.html#the-t-distribution-degrees-of-freedom",
    "href": "Probability.html#the-t-distribution-degrees-of-freedom",
    "title": "Statistical Theory",
    "section": "The t Distribution & Degrees of Freedom",
    "text": "The t Distribution & Degrees of Freedom\nThe shape of the distribution used to compare the test-statistic with depends on the degrees of freedom.\nIt basically indicates how many values are free to vary, once you know e.g. the mean. If you have three exam scores (70, 80, 90), and you know the mean (80) and two scores (70 and 80), then the third score is already fixed to be 90.\nIn this case, there would be n-1 = 2 df.",
    "crumbs": [
      "Statistical Theory"
    ]
  },
  {
    "objectID": "Probability.html#determine-the-probability-of-the-observed-result-under-the-null-hypothesis",
    "href": "Probability.html#determine-the-probability-of-the-observed-result-under-the-null-hypothesis",
    "title": "Statistical Theory",
    "section": "Determine the Probability of the Observed Result under the Null Hypothesis",
    "text": "Determine the Probability of the Observed Result under the Null Hypothesis\nWe do not check likelihood of the alternative distribution or likelihood that the null hypothesis is true, but rather:\nHow likely is it, given that we assume \\(H_0\\) is true, to observe a statistic at least as extreme as the one we observed.\n⇒ We need to know the distribution of the expected statistic, assuming \\(H_0\\) is true. Then we can calculate how (un-)likely it is to find the statistic (or a more extreme value) we found in our data.",
    "crumbs": [
      "Statistical Theory"
    ]
  },
  {
    "objectID": "Probability.html#the-p-value",
    "href": "Probability.html#the-p-value",
    "title": "Statistical Theory",
    "section": "The P-Value",
    "text": "The P-Value\nIn Jamovi, we get a p-value, which is the probability under the curve to the right of the red line on the previous slide (as or more extreme than test statistic).\nIf this p-value is smaller than the ⍺ = 0.05 defined above, we usually reject the null hypothesis.\nIt tells us that it is relatively unlikely to find a difference between caffeine intake groups that big, if caffeine actually had no impact on cognitive performance/exam scores.",
    "crumbs": [
      "Statistical Theory"
    ]
  },
  {
    "objectID": "Probability.html#what-does-a-significant-result-not-mean",
    "href": "Probability.html#what-does-a-significant-result-not-mean",
    "title": "Statistical Theory",
    "section": "What does a significant result (not) mean?",
    "text": "What does a significant result (not) mean?\nThere is a lot of discussion about the usefulness of using \\(\\alpha = .05\\) as well as about the interpretation of a significant result/certain p-value!\n\nA p-value of .01 does….\n\nNOT mean that the probability that \\(H_0\\) is true is 1%!\n\nWe tested \\(P(data|H_0)\\) not \\(P(H_0|data)\\)!\n\nNOT mean that the probability that you’re making a wrong decision is 1%!\n\nThis would also be \\(P(H_0|data)\\)! p-values are probabilities of data (under \\(H_0\\)), not probabilities of hypotheses!\n\nNOT mean that you would get the same significance 99% of the time if you repeated the study.\n\nThe p-value is a statement about the likelihood of one particular dataset under the null.\n\nNOT mean that you found a practically important effect.\n\nDifference between statistical significance and practical significance! Effect sizes are important here. (Statistical significance depends on sample size!)",
    "crumbs": [
      "Statistical Theory"
    ]
  },
  {
    "objectID": "Probability.html#power-effect-sizes",
    "href": "Probability.html#power-effect-sizes",
    "title": "Statistical Theory",
    "section": "Power & Effect Sizes",
    "text": "Power & Effect Sizes\nEarlier, I mentioned the Type II error (𝛽, false negative: accepting \\(H_0\\) when there is actually an effect).\nI also just briefly mentioned that we try to minimize that error as well, but couldn’t fix it like the Type I error. However, we can try to minimize it by increasing the power (𝛽 - 1) of a study. The power indicates how well your study is able to find an effect that is actually there.\n\nPower can be increased by:\n\nincreasing sample size (which decreases variance, SE, of the sample distribution)\nreducing measurement error\na high effect size\n\n\n\nEffect sizes give an estimate of, well, the size of the effect you found. They are important for two reasons:\n\nReporting effect sizes: It’s good practice to report them, so others know whether the effect is not only statistically but also practically significant. There are various measures of effect size (beyond the scope of these workshop); in Jamovi it is easy to simply check “effect size”.\nCalculating the (a priori) power of your study: Running a power analysis is very important while planning your study, as it allows you to either calculate the necessary sample size for attaining a certain level of power (e.g. 80%) or calculating the power for a certain sample size.",
    "crumbs": [
      "Statistical Theory"
    ]
  }
]